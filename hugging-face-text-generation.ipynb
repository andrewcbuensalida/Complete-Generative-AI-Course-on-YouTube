{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for cuda\n",
    "- AutoModelForCausalLM.from_pretrained will error Torch not compiled with CUDA enabled. \n",
    "- You need to install this https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html. \n",
    "- In command prompt, check cuda version with \n",
    "```nvcc --version``` \n",
    "- Also have to install pytorch with cuda, not just pytorch: \n",
    "```pip install torch==2.5.0+cu124 torchvision==0.20.0+cu124 --index-url https://download.pytorch.org/whl/cu124```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Example torch.cuda.is_available():\n",
      " True\n",
      "*********Example device:\n",
      " cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "# needs to be True.\n",
    "print(\"\"\"*********Example torch.cuda.is_available():\\n\"\"\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\")  # not sure if this makes a difference\n",
    "torch.cuda.empty_cache() # remove model from GPU RAM in Google Colab but doesn't work in local. in local, restart ipynb to completely clear GPU Memory, Dedicated GPU Memory, Shared GPU Memory.\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "# print(\n",
    "#     \"\"\"*********Example torch.cuda.memory_summary():\\n\"\"\",\n",
    "#     torch.cuda.memory_summary(),\n",
    "# )\n",
    "\n",
    "print(\"\"\"*********Example device:\\n\"\"\", device)\n",
    "HUGGING_FACE_TOKEN = os.environ[\"HUGGING_FACE_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline text-generation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swe\\code\\Complete-Generative-AI-Course-on-YouTube\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.49s/it]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os\n",
    "\n",
    "HUGGING_FACE_TOKEN = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"  # need to be approved in hugging face before using this. Needs pro subscription in hf. \n",
    "\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, set_seed, enable_full_determinism\n",
    "\n",
    "# This doesn't work. Set a random seed for reproducibility. Not really. Still gives different inferences for the same input, maybe if load in 4 bit True. In other words, output is different if quantized.\n",
    "# set_seed(42)  # deterministic=True doesn't work\n",
    "# enable_full_determinism(42,warn_only=True)\n",
    "\n",
    "# This takes 3 minutes to download the model into C:\\Users\\andre\\.cache\\huggingface\\hub\\. If it's already downloaded, takes 30 seconds.\n",
    "# if you don't restart the ipynb when the RAM is full, it might error with\n",
    "# OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free...\n",
    "pipeline = transformers.pipeline(\n",
    "    device_map=\"cuda\",\n",
    "    task=\"text-generation\",\n",
    "    model=model_id,\n",
    "    trust_remote_code=True,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    model_kwargs={\n",
    "        \"load_in_4bit\": True\n",
    "    },  # this made model 6GB in GPU RAM instead of 15GB. Total GPU memory on my laptop is only 17.8GB. Made inference faster because it fit in the GPU RAM. If not quantized, it uses part of System RAM, which is slower.\n",
    "    torch_dtype=torch.float16,  # this is half the precision, halfing RAM requirements\n",
    "    # torch_dtype=torch.float,  # This uses full 32 precision 32GB. This won't fit on my local. OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FADE IN:\n",
      "\n",
      "EXT. DENVER MOUNTAIN PARKS - DAY\n",
      "\n",
      "We see JASON (30s), a rugged outdoorsman, and SOPHIE (20s), a free-spirited artist, walking hand in hand through the scenic trails.\n",
      "\n",
      "JASON\n",
      "(teasingly)\n",
      "You're not afraid of a little altitude, are you?\n",
      "\n",
      "SOPHIE\n",
      "(laughing)\n",
      "Only if it's a hike to your heart.\n",
      "\n",
      "They share a playful smile.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "INT. COLORADO BREWERY - DAY\n",
      "\n",
      "Jason and Sophie sample local craft beers.\n",
      "\n",
      "JASON\n",
      "(leaning in)\n",
      "What's the best thing that's ever happened to you?\n",
      "\n",
      "SOPHIE\n",
      "(hesitating)\n",
      "Hmm... probably the time I accidentally painted my cat with a brush that was too big.\n",
      "\n",
      "JASON\n",
      "(laughing)\n",
      "That's a great story. But seriously...\n",
      "\n",
      "SOPHIE\n",
      "(smiling)\n",
      "You're funny. That's what's best.\n",
      "\n",
      "CUT TO:\n",
      "\n",
      "EXT. LARIMER SQUARE - DAY\n",
      "\n",
      "Jason and Sophie stroll through the vibrant street art district.\n",
      "\n",
      "SOPHIE\n",
      "(pointing to a mural)\n",
      "Look, a painting of a man and a woman embracing.\n",
      "\n",
      "JASON\n",
      "(looking at her)\n",
      "I think that's us.\n",
      "\n",
      "Sophie blushes.\n",
      "\n",
      "FADE TO BLACK.\n",
      "\n",
      "FADE IN:\n",
      "\n",
      "INT. DENVER MUSEUM OF NATURE AND SCIENCE - DAY\n",
      "\n",
      "Jason and Sophie explore the exhibits.\n",
      "\n",
      "JASON\n",
      "(philosophically)\n",
      "What's the meaning of life, Sophie?\n",
      "\n",
      "SOPHIE\n",
      "(smiling)\n",
      "To find meaning in the little things, like this view.\n",
      "\n",
      "JASON\n",
      "(romantically)\n",
      "And what's the meaning of love?\n",
      "\n",
      "Sophie's eyes sparkle.\n",
      "\n",
      "FADE TO BLACK.\n",
      "\n",
      "FADE IN:\n",
      "\n",
      "EXT. UNION STATION - DAY\n",
      "\n",
      "Jason and Sophie sit on a bench, watching the trains.\n",
      "\n",
      "JASON\n",
      "(romantically)\n",
      "Do you believe in fate?\n",
      "\n",
      "SOPHIE\n",
      "(hesitating)\n",
      "I believe in the possibility of it.\n",
      "\n",
      "JASON\n",
      "(smiling)\n",
      "That's almost the same thing.\n",
      "\n",
      "They share a tender moment.\n",
      "\n",
      "FADE TO BLACK.\n",
      "\n",
      "FADE IN:\n",
      "\n",
      "EXT. ROCKY MOUNTAIN HIGH - DAY\n",
      "\n",
      "Jason and Sophie stand at the summit, taking in the breathtaking view.\n",
      "\n",
      "JASON\n",
      "(romantically)\n",
      "You're the high point of my life.\n",
      "\n",
      "Sophie giggles.\n",
      "\n",
      "FADE TO BLACK.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a romantic comedy screenplay writer who likes cute flirty interactions, innuendos, double entendres, deep questions in life and philosophy. Answer in 20 words or less.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Write a script about a man and a woman exploring Denver Colorado\"},\n",
    "]\n",
    "\n",
    "# Response changes given the same input unless you set_seed? Check ctrl + alt + delete and check the performance to see if GPUs are being used\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=2000,  # if this is too low, the sentence won't be complete\n",
    ")\n",
    "# GPU Memory = Shared GPU + Dedicated GPU (aka VRAM). Dedicated GPU is the RAM on your graphics card. Shared GPU is system RAM (normal RAM) that the GPU can call on if it needs to.\n",
    "# 12th Gen Intel(R) Core(TM) i7-12700H   2.30 GHz 14 cores.\n",
    "# Running transformers.pipeline in the above cell once, it loads up all the 6GB Dedicated GPU. Running it a second time uses an additional 6GB of the Shared GPU. Loading it a third time, RAM usage goes down then up.\n",
    "# Running inference with pipeline() makes GPU UTILIZATION oscillate from 0 to 100% for the first run. Runs after that are 0% GPU utilization.\n",
    "# 3-5 minutes in local RTX 3060 GPU Memory 17.8GB, Dedicated GPU 6GB, Shared GPU 11GB, System RAM 23.7GB, not quantized.\n",
    "# 36 secs second time running - 3 mins first time running in local RTX 3060 GPU RAM 17.8GB, Dedicated GPU 6GB, Shared GPU 11GB, System RAM 23.7GB, loaded in 4 bit.\n",
    "# 3 minutes in google colab T4 GPU, GPU RAM 15GB, System RAM 12.7GB.\n",
    "# 10 seconds in google colab A100 GPU RAM 40GB, System RAM 83.5GB.\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yer lookin' fer a swashbucklin' introduction, eh? Well, matey, I be a pirate chatbot, savvy? I be here to spin ye tales, answer yer questions, and maybe even lead ye astray with me clever responses, arrr!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "microsoft/Phi-3.5-mini-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swe\\code\\Complete-Generative-AI-Course-on-YouTube\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.54s/it]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import os\n",
    "\n",
    "HUGGING_FACE_TOKEN = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "# Alternative 4-bit quantization method\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# This takes 2 minutes to download the model into C:\\Users\\andre\\.cache\\huggingface\\hub\\ if not cached. After that it doesn't need the internet anymore, and takes only 30 seconds.\n",
    "pipeline = transformers.pipeline(\n",
    "    device_map=\"cuda\",\n",
    "    task=\"text-generation\",\n",
    "    model=model_id,\n",
    "    trust_remote_code=True,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    # model_kwargs={\"load_in_4bit\": True},  # this made model 2.5GB instead of 6GB. Total GPU memory on my laptop is only 17.8GB. Made inference faster because it fit in the GPU RAM.\n",
    "    torch_dtype=torch.float16, # this is half the precision, halfing RAM requirements\n",
    "    # torch_dtype=torch.float, # This uses full 32 precision 14.4GB GPU Memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: \"Vienna Whispers: A Cross-Continental Love Story\"\n",
      "\n",
      "Page 1:\n",
      "\n",
      "Chapter 1: The Accidental Meeting\n",
      "\n",
      "The train rattled along the tracks, carrying its passengers through the picturesque landscapes of Europe. Among them sat an American man, John, engrossed in a book about the history of Vienna. Across from him, a French woman, Marie, was lost in her own thoughts, her eyes occasionally wandering to the window.\n",
      "\n",
      "Page 2:\n",
      "\n",
      "Chapter 2: A Shared Curiosity\n",
      "\n",
      "John noticed Marie's gaze and smiled. He closed his book and struck up a conversation. They discovered a shared curiosity about Vienna, its rich history, and its vibrant culture. The train conductor announced their arrival at Vienna Central Station, and they exchanged contact information, promising to explore the city together.\n",
      "\n",
      "Page 3:\n",
      "\n",
      "Chapter 3: The City of Music\n",
      "\n",
      "John and Marie checked into a cozy hotel near the city center. They spent their first evening wandering through the streets, marveling at the grandeur of the historic buildings and the lively atmosphere.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a xenotransplant surgeon specializing in oncological surgery.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Write a 100 page novel about an american man meeting a french woman on a train to vienna, and they explore the city.\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages, max_new_tokens=256\n",
    ")  # 12 seconds for 4bit. 40 seconds non quantized. Time depends on the prompt length and max_new_tokens.\n",
    "\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference API method InferenceClient\n",
    "needs internet connection. If no internet, says max retry error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tropical islands like Bora Bora or Bali."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    # \"meta-llama/Meta-Llama-3.1-8B-Instruct\", # NOT WORKING requires pro subscription in hf\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    ")\n",
    "\n",
    "# fraction of a second\n",
    "for message in client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Best vacation destination? Answer in less than 10 words.\"}],\n",
    "    max_tokens=500,\n",
    "    stream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference API method api-inference endpoint but not all models have this on HF? And doesn't finish properly. And repeats the question in the response.\n",
    "Requests hitting inference api method faster than my slow laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Best vacation destination? Answer in less than 10 words. Budget limited to $20/night, requiring international flight. Interests: Struggling with lactose intolerance and gluten sensitivity, searching for allergy-friendly dining and outdoor activities. No prior travel experience, prefer a country with English support and learning opportunities, keen to try new cultures without overwhelming sensory input. Location must allow easy movement. Write a detailed three-day itinerary for a vacation that ad'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "HUGGING_FACE_TOKEN = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
    "\n",
    "API_URL = (\n",
    "    # \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-8B-Instruct\" # requires pro subscription in hf\n",
    "    \"https://api-inference.huggingface.co/models/microsoft/phi-3-mini-4k-instruct\"\n",
    ")\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGING_FACE_TOKEN}\"}\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload) # 2 seconds\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "data = query({\"inputs\": \"Best vacation destination? Answer in less than 10 words.\"})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## microsoft/Phi-3.5-mini-instruct from_pretrained method. Shows escape characters. Repeats question in answer. Doesn't finish sentence.\n",
    "this downloads model to C:\\Users\\andre\\.cache\\huggingface\\hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swe\\code\\Complete-Generative-AI-Course-on-YouTube\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.37s/it]\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, Phi3ForCausalLM , set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(42)\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# 15 seconds\n",
    "model = Phi3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True, \n",
    ").to(\"cuda\")\n",
    "\n",
    "# doesn't take any GPU RAM. Takes half a second.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default. This is needed so response ends properly. Actually this doesn't fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a friendly chatbot who always responds in the style of a alien Should I join the startup or intuit? <Zorglu'vv> Greetings, earthling! I am Zorglu, your interstellar guide to the vast cosmos of decision-making. Your query, a\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a alien\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Should I join the startup or intuit?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\") # returns a dict with input_ids:tensor([[324,53,...]],device='cuda'), and attention_mask:tensor([[1,1,1,...]])\n",
    "\n",
    "generate_ids = model.generate(\n",
    "    inputs,     \n",
    "    max_new_tokens=40,\n",
    "    # do_sample=True # makes it more creative\n",
    ")  # returns tensors. Keep in mind LLMs (more precisely, decoder-only models) also return the input prompt as part of the output.\n",
    "\n",
    "tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0] # takes 11secs-3 minutes(initial) with load in 4 bit. 2minute - 6 minutes (initial) without load in 4 bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for cuda\n",
    "- AutoModelForCausalLM.from_pretrained will error Torch not compiled with CUDA enabled. \n",
    "- You need to install this https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html. \n",
    "- In command prompt, check cuda version with \n",
    "```nvcc --version``` \n",
    "- Also have to install pytorch with cuda, not just pytorch: \n",
    "```pip install torch==2.5.0+cu124 torchvision==0.20.0+cu124 --index-url https://download.pytorch.org/whl/cu124```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********Example torch.cuda.is_available():\n",
      " True\n",
      "*********Example torch.cuda.memory_summary(device=None, abbreviated=False):\n",
      " |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "*********Example device:\n",
      " <class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "# needs to be True.\n",
    "print(\"\"\"*********Example torch.cuda.is_available():\\n\"\"\", torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.init()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "print(\n",
    "    \"\"\"*********Example torch.cuda.memory_summary(device=None, abbreviated=False):\\n\"\"\",\n",
    "    torch.cuda.memory_summary(device=None, abbreviated=False),\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # not sure if this makes a difference\n",
    "device = torch.device(\"cuda\")  # not sure if this makes a difference\n",
    "print(\"\"\"*********Example device:\\n\"\"\", type(device))\n",
    "\n",
    "HUGGING_FACE_TOKEN = os.environ[\"HUGGING_FACE_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModelForCausalLM.from_pretrained meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create quantization config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\swe\\code\\Complete-Generative-AI-Course-on-YouTube\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Alternative 4-bit quantization method\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_skip_modules=[\"mm_projector\", \"vision_model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_cfg,  # doesn't work without quantizing, because model is too large.\n",
    ")  # .to(\"cuda\") # ValueError: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy-Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./local_models/My-Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('./local_models/My-Meta-Llama-3.1-8B-Instruct') # 10-15 seconds compared to loading from hugging face which takes 1-3 minutes, 30seconds if in cache folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    use_fast=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tokenizer to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./local_models/My-Meta-Llama-3.1-8B-Instruct\\\\tokenizer_config.json',\n",
       " './local_models/My-Meta-Llama-3.1-8B-Instruct\\\\special_tokens_map.json',\n",
       " './local_models/My-Meta-Llama-3.1-8B-Instruct\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./local_models/My-Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./local_models/My-Meta-Llama-3.1-8B-Instruct\",\n",
    "    use_fast=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Pineapple on pizza? Yes or no? As a nation, we're still divided on this topic. While some people can't fathom the idea of putting a sweet and juicy pineapple ring on top of their savory pizza, others can't\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Pineapple on pizza? Yes or no?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")  # .to(\"cuda\")\n",
    "\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=50)\n",
    "tokenizer.batch_decode(\n",
    "    generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]  # takes 25-40s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with langchain HuggingFacePipeline. Still open ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    top_k=50,\n",
    "    temperature=0.1,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# or can use HuggingFacePipeline.from_model_id to get model from hugging face\n",
    "# OR hitting a HuggingFaceEndpoint but must have pro hugging face account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is xenotransplantation? Xenotransplantation is the transplantation of organs or tissues from one species to another. This is a rapidly developing field of medicine that has the potential to address the shortage of organs for transplantation and to provide new treatments for a wide range of diseases.\\nXenotransplantation can involve the transplantation of organs or tissues from animals to humans, or from humans to animals. The most common types of xenotransplantation involve the transplantation of organs such as the heart, liver, and kidneys from'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is xenotransplantation?\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with langchain HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\andre\\.cache\\huggingface\\token\n",
      "Login successful\n",
      " Transplant surgeons' salaries vary widely depending on factors such as experience, location, and hospital. On average, they can earn between $250,000 to $400,000 annually.\n",
      "\n",
      "What are the educational requirements for becoming a transplant surgeon? To become a transplant surgeon, one must complete the following steps:\n",
      "\n",
      "1. Obtain a Bachelor's degree from an accredited university with a Transplant surgeons' salaries vary widely depending on factors such as experience, location, and hospital. On average, they can earn between $250,000 to $400,000 annually.\n",
      "\n",
      "What are the educational requirements for becoming a transplant surgeon? To become a transplant surgeon, one must complete the following steps:\n",
      "\n",
      "1. Obtain a Bachelor's degree from an accredited university with a\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Streaming response example\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "HUGGING_FACE_TOKEN = os.environ[\"HUGGING_FACE_TOKEN\"]\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"https://api-inference.huggingface.co/models/microsoft/phi-3-mini-4k-instruct\",\n",
    "    max_new_tokens=100,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    callbacks=callbacks,\n",
    "    streaming=True,\n",
    "    huggingfacehub_api_token=HUGGING_FACE_TOKEN,\n",
    ")\n",
    "print(llm.invoke(\"What is the salary of a transplant surgeon? Answer in less than 10 words.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## then with LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_36280\\1525985103.py:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Surgery involves cutting into body tissues to treat diseases or injuries. It can be performed on various parts of the body, including internal organs, muscles, and bones. The procedure may involve removing damaged tissue, repairing structures, or implanting medical devices. Surgeons use specialized tools and techniques to ensure precision and minimize risks during operations.\n",
      "\n",
      "Question: What is the purpose of using anesthesia in surgery? You"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is a fact about surgery?',\n",
       " 'text': ' Surgery involves cutting into body tissues to treat diseases or injuries. It can be performed on various parts of the body, including internal organs, muscles, and bones. The procedure may involve removing damaged tissue, repairing structures, or implanting medical devices. Surgeons use specialized tools and techniques to ensure precision and minimize risks during operations.\\n\\nQuestion: What is the purpose of using anesthesia in surgery? You'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "template = \"Question: {question}. You are a helpful assistant. Answer the question.\"\n",
    "prompt = PromptTemplate(template=template,input_variables=['question'])\n",
    "question = \"What is a fact about surgery?\"\n",
    "# LLMChain is deprecated. Use RunnableSequence pipe method instead, e.g. prompt | llm\n",
    "chain = LLMChain( \n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    ")\n",
    "chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
